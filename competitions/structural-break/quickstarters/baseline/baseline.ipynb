{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvWIItAe-0fN"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/crunchdao/quickstarters/blob/master/competitions/structural-break/quickstarters/baseline/baseline.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNUXnJa_-0fO"
      },
      "source": [
        "![Banner](https://raw.githubusercontent.com/crunchdao/quickstarters/refs/heads/master/competitions/structural-break/assets/banner.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lurIF1Ve-0fP"
      },
      "source": [
        "# ADIA Lab Structural Break Challenge\n",
        "\n",
        "## Challenge Overview\n",
        "\n",
        "Welcome to the ADIA Lab Structural Break Challenge! In this challenge, you will analyze univariate time series data to determine whether a structural break has occurred at a specified boundary point.\n",
        "\n",
        "### What is a Structural Break?\n",
        "\n",
        "A structural break occurs when the process governing the data generation changes at a certain point in time. These changes can be subtle or dramatic, and detecting them accurately is crucial across various domains such as climatology, industrial monitoring, finance, and healthcare.\n",
        "\n",
        "![Structural Break Example](https://raw.githubusercontent.com/crunchdao/competitions/refs/heads/master/competitions/structural-break/quickstarters/baseline/images/example.png)\n",
        "\n",
        "### Your Task\n",
        "\n",
        "For each time series in the test set, you need to predict a score between `0` and `1`:\n",
        "- Values closer to `0` indicate no structural break at the specified boundary point;\n",
        "- Values closer to `1` indicate a structural break did occur.\n",
        "\n",
        "### Evaluation Metric\n",
        "\n",
        "The evaluation metric is [ROC AUC (Area Under the Receiver Operating Characteristic Curve)](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html), which measures the performance of detection algorithms regardless of their specific calibration.\n",
        "\n",
        "- ROC AUC around `0.5`: No better than random chance;\n",
        "- ROC AUC approaching `1.0`: Perfect detection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-UrlmpOMnQw"
      },
      "source": [
        "# Setup\n",
        "\n",
        "The first steps to get started are:\n",
        "1. Get the setup command\n",
        "2. Execute it in the cell below\n",
        "\n",
        "### >> https://hub.crunchdao.com/competitions/structural-break/submit/notebook\n",
        "\n",
        "![Reveal token](https://raw.githubusercontent.com/crunchdao/competitions/refs/heads/master/documentation/animations/reveal-token.gif)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DUeixiC_IJM",
        "outputId": "563978e8-300b-422a-f11a-c7362357d138"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "crunch-cli, version 6.5.0\n",
            "you appear to have never submitted code before\n",
            "data/X_train.parquet: download from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/146/X_train.parquet (204327238 bytes)\n",
            "data/X_test.reduced.parquet: download from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/146/X_test.reduced.parquet (2380918 bytes)\n",
            "data/y_train.parquet: download from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/146/y_train.parquet (61003 bytes)\n",
            "data/y_test.reduced.parquet: download from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/146/y_test.reduced.parquet (2655 bytes)\n",
            "                                \n",
            "---\n",
            "Success! Your environment has been correctly setup.\n",
            "Next recommended actions:\n",
            "1. Load the Crunch Toolings: `crunch = crunch.load_notebook()`\n",
            "2. Execute the cells with your code\n",
            "3. Run a test: `crunch.test()`\n",
            "4. Download and submit your code to the platform!\n"
          ]
        }
      ],
      "source": [
        "%pip install crunch-cli --upgrade --quiet --progress-bar off\n",
        "!crunch setup-notebook structural-break 9KnBOg5xU4qvMxIOrbROU6Ou"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import ttest_ind\n",
        "from scipy.stats import t\n",
        "import time\n",
        "from multiprocessing import Pool, cpu_count\n",
        "from scipy.stats import shapiro\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from statsmodels.graphics.tsaplots import plot_acf,plot_pacf\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from scipy.stats import skew, kurtosis, iqr, ttest_ind, ks_2samp\n",
        "# from pykalman import KalmanFilter\n",
        "import scipy\n",
        "\n",
        "import joblib\n",
        "import typing\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "\n",
        "# ---------------- Custom Transformer ----------------\n",
        "from multiprocessing import Pool, cpu_count"
      ],
      "metadata": {
        "id": "utRx8ZFoNM9F"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IBhw7hv-0fQ"
      },
      "source": [
        "# Your model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpLeMWSw-0fQ"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-18T09:52:21.302334Z",
          "start_time": "2024-11-18T09:52:18.268241Z"
        },
        "id": "MKqz-6Zw-0fR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import typing\n",
        "\n",
        "# Import your dependencies\n",
        "import joblib\n",
        "import pandas as pd\n",
        "import scipy\n",
        "import sklearn.metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjD_WSAS-0fR",
        "outputId": "ad5ee1e1-937b-475e-f70c-9e1bb950493c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loaded inline runner with module: <module '__main__'>\n",
            "\n",
            "cli version: 6.5.0\n",
            "available ram: 12.67 gb\n",
            "available cpu: 2 core\n",
            "----\n"
          ]
        }
      ],
      "source": [
        "import crunch\n",
        "\n",
        "# Load the Crunch Toolings\n",
        "crunch = crunch.load_notebook()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiKJODFx-0fR"
      },
      "source": [
        "## Understanding the Data\n",
        "\n",
        "The dataset consists of univariate time series, each containing ~2,000-5,000 values with a designated boundary point. For each time series, you need to determine whether a structural break occurred at this boundary point.\n",
        "\n",
        "The data was downloaded when you setup your local environment and is now available in the `data/` directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKHXgvjN-0fS",
        "outputId": "2720dced-45ea-419f-a462-b7f68fc927b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data/X_train.parquet: download from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/146/X_train.parquet (204327238 bytes)\n",
            "data/X_train.parquet: already exists, file length match\n",
            "data/X_test.reduced.parquet: download from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/146/X_test.reduced.parquet (2380918 bytes)\n",
            "data/X_test.reduced.parquet: already exists, file length match\n",
            "data/y_train.parquet: download from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/146/y_train.parquet (61003 bytes)\n",
            "data/y_train.parquet: already exists, file length match\n",
            "data/y_test.reduced.parquet: download from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/146/y_test.reduced.parquet (2655 bytes)\n",
            "data/y_test.reduced.parquet: already exists, file length match\n"
          ]
        }
      ],
      "source": [
        "# Load the data simply\n",
        "X_train, y_train, X_test = crunch.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7T_JmgMq-0fS"
      },
      "source": [
        "### Understanding `X_train`\n",
        "\n",
        "The training data is structured as a pandas DataFrame with a MultiIndex:\n",
        "\n",
        "**Index Levels:**\n",
        "- `id`: Identifies the unique time series\n",
        "- `time`: The timestep within each time series\n",
        "\n",
        "**Columns:**\n",
        "- `value`: The actual time series value at each timestep\n",
        "- `period`: A binary indicator where `0` represents the **period before** the boundary point, and `1` represents the **period after** the boundary point"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "0oRCTnOb-0fS",
        "outputId": "2d0663ba-76b2-4937-d7fc-e6c314784242"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>value</th>\n",
              "      <th>period</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th>time</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
              "      <th>0</th>\n",
              "      <td>0.001858</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.001664</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.004386</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000699</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.002433</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"5\" valign=\"top\">10000</th>\n",
              "      <th>1890</th>\n",
              "      <td>-0.005903</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1891</th>\n",
              "      <td>0.007295</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1892</th>\n",
              "      <td>0.003527</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1893</th>\n",
              "      <td>0.007218</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1894</th>\n",
              "      <td>0.000034</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>23802099 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "               value  period\n",
              "id    time                  \n",
              "0     0     0.001858       0\n",
              "      1    -0.001664       0\n",
              "      2    -0.004386       0\n",
              "      3     0.000699       0\n",
              "      4    -0.002433       0\n",
              "...              ...     ...\n",
              "10000 1890 -0.005903       1\n",
              "      1891  0.007295       1\n",
              "      1892  0.003527       1\n",
              "      1893  0.007218       1\n",
              "      1894  0.000034       1\n",
              "\n",
              "[23802099 rows x 2 columns]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WP39dgx-0fS"
      },
      "source": [
        "### Understanding `y_train`\n",
        "\n",
        "This is a simple `pandas.Series` that tells if a dataset id has a structural breakpoint or not.\n",
        "\n",
        "**Index:**\n",
        "- `id`: the ID of the dataset\n",
        "\n",
        "**Value:**\n",
        "- `structural_breakpoint`: Boolean indicating whether a structural break occurred (`True`) or not (`False`)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oSS08Ks-0fT"
      },
      "source": [
        "### Understanding `X_test`\n",
        "\n",
        "The test data is provided as a **`list` of `pandas.DataFrame`s** with the same format as [`X_train`](#understanding-X_test).\n",
        "\n",
        "It is structured as a list to encourage processing records one by one, which will be mandatory in the `infer()` function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgulFOGX-0fT"
      },
      "source": [
        "## Strategy Implementation\n",
        "\n",
        "There are multiple approaches you can take to detect structural breaks:\n",
        "\n",
        "1. **Statistical Tests**: Compare distributions before and after the boundary point;\n",
        "2. **Feature Engineering**: Extract features from both segments for comparison;\n",
        "3. **Time Series Modeling**: Detect deviations from expected patterns;\n",
        "4. **Machine Learning**: Train models to recognize break patterns from labeled examples.\n",
        "\n",
        "The baseline implementation below uses a simple statistical approach: a t-test to compare the distributions before and after the boundary point."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLfYIXlz-0fT"
      },
      "source": [
        "### The `train()` Function\n",
        "\n",
        "In this function, you build and train your model for making inferences on the test data. Your model must be stored in the `model_directory_path`.\n",
        "\n",
        "The baseline implementation below doesn't require a pre-trained model, as it uses a statistical test that will be computed at inference time."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "indices_x = X_train.index.get_level_values(0).unique().tolist()\n",
        "indices_x.sort()"
      ],
      "metadata": {
        "id": "E_N2uz3UOBxM"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def kalman_stats(series):\n",
        "    kf = KalmanFilter(initial_state_mean=0, n_dim_obs=1)\n",
        "    # fewer EM iterations\n",
        "    kf = kf.em(series, n_iter=1)\n",
        "    state_means, _ = kf.smooth(series)\n",
        "    state_means = state_means.flatten()\n",
        "\n",
        "    smoothed_mean = np.mean(state_means)\n",
        "    smoothed_std = np.std(state_means)\n",
        "    residual = series - state_means\n",
        "    residual_std = np.std(residual)\n",
        "    trend_strength = smoothed_std / (np.std(series) + 1e-6)\n",
        "\n",
        "    return {\n",
        "        'kf_mean': smoothed_mean,\n",
        "        'kf_std': smoothed_std,\n",
        "        'kf_residual_std': residual_std,\n",
        "        'kf_trend_strength': trend_strength\n",
        "    }\n",
        "\n",
        "def fast_ema(x, span):\n",
        "    alpha = 2/(span+1)\n",
        "    ema = np.empty_like(x)\n",
        "    ema[0] = x[0]\n",
        "    for i in range(1, len(x)):\n",
        "        ema[i] = alpha * x[i] + (1-alpha) * ema[i-1]\n",
        "    return ema[-1]\n",
        "\n",
        "def extract_features(a, b):\n",
        "    features = {}\n",
        "\n",
        "    # Basic statistics\n",
        "    features['mean_a'] = np.mean(a)\n",
        "    features['mean_b'] = np.mean(b)\n",
        "    features['std_a'] = np.std(a)\n",
        "    features['std_b'] = np.std(b)\n",
        "    features['skew_a'] = skew(a)\n",
        "    features['skew_b'] = skew(b)\n",
        "    features['kurt_a'] = kurtosis(a)\n",
        "    features['kurt_b'] = kurtosis(b)\n",
        "    features['iqr_a'] = iqr(a)\n",
        "    features['iqr_b'] = iqr(b)\n",
        "    features['std_ratio'] = features['std_b'] / (features['std_a'] + 1e-10)  # avoid div0\n",
        "\n",
        "    # Fit t-distribution (consider sampling if large arrays)\n",
        "    params_a = scipy.stats.t.fit(a)\n",
        "    params_b = scipy.stats.t.fit(b)\n",
        "    features['t_val_a'] = params_a[0]\n",
        "    features['t_mu_a'] = params_a[1]\n",
        "    features['t_sigma_a'] = params_a[2]\n",
        "    features['t_val_b'] = params_b[0]\n",
        "    features['t_mu_b'] = params_b[1]\n",
        "    features['t_sigma_b'] = params_b[2]\n",
        "\n",
        "    # Differences\n",
        "    features['mean_diff'] = features['mean_b'] - features['mean_a']\n",
        "    features['std_diff'] = features['std_b'] - features['std_a']\n",
        "\n",
        "    # Statistical tests\n",
        "    features['ttest_p'] = ttest_ind(a, b, equal_var=False).pvalue\n",
        "    features['ks_p'] = ks_2samp(a, b).pvalue\n",
        "\n",
        "    # Kalman features\n",
        "    # try:\n",
        "\n",
        "    #   kf_a = kalman_stats(np.array(a))\n",
        "    #   kf_b = kalman_stats(np.array(b))\n",
        "    #   for k, v in kf_a.items():\n",
        "    #       features[f'{k}_a'] = v\n",
        "    #   for k, v in kf_b.items():\n",
        "    #       features[f'{k}_b'] = v\n",
        "    #   features['kf_trend_strength_diff'] = features['kf_trend_strength_b'] - features['kf_trend_strength_a']\n",
        "\n",
        "    # except:\n",
        "    #   pass\n",
        "\n",
        "    # EMA features (use fast_ema)\n",
        "    ema_windows = [5, 10, 15, 20, 25]\n",
        "    for w in ema_windows:\n",
        "        ema_a = fast_ema(a, w)\n",
        "        ema_b = fast_ema(b, w)\n",
        "        features[f'ema{w}_a'] = ema_a\n",
        "        features[f'ema{w}_b'] = ema_b\n",
        "        features[f'ema{w}_diff'] = ema_b - ema_a\n",
        "\n",
        "    # Cross-correlation\n",
        "    min_len = min(len(a), len(b))\n",
        "    a = a[:min_len]\n",
        "    b = b[:min_len]\n",
        "    max_lag = min(10, min_len - 1)\n",
        "    xcorr = []\n",
        "    for lag in range(1, max_lag + 1):\n",
        "        corr = np.corrcoef(a[:-lag], b[lag:])[0, 1]\n",
        "        xcorr.append(corr)\n",
        "\n",
        "    features['max_xcorr'] = np.nanmax(xcorr)\n",
        "    features['mean_xcorr'] = np.nanmean(xcorr)\n",
        "\n",
        "    return features\n"
      ],
      "metadata": {
        "id": "xlJohLzrOEew"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-18T10:04:00.459399Z",
          "start_time": "2024-11-18T10:04:00.455716Z"
        },
        "id": "xQwWDC6M-0fT"
      },
      "outputs": [],
      "source": [
        "def process_chunk(temp):\n",
        "  t0_series = temp[temp.period == 0].value.values\n",
        "  t1_series = temp[temp.period == 1].value.values\n",
        "  features_v = extract_features(t0_series, t1_series)\n",
        "  return features_v\n",
        "\n",
        "\n",
        "\n",
        "def train(\n",
        "    X_train: pd.DataFrame,\n",
        "    y_train: pd.Series,\n",
        "    model_directory_path: str,\n",
        "):\n",
        "    os.makedirs(model_directory_path, exist_ok=True)\n",
        "\n",
        "    # try:\n",
        "    #   !pip install pykalman\n",
        "    # except:\n",
        "    #   pass\n",
        "\n",
        "\n",
        "\n",
        "    # For our baseline t-test approach, we don't need to train a model\n",
        "    # This is essentially an unsupervised approach calculated at inference time\n",
        "    data_file = os.path.join(model_directory_path,\"data.csv\")\n",
        "\n",
        "      index_x = X_train.index.get_level_values(0).unique().tolist()\n",
        "      features_list = []\n",
        "      for i in index_x:\n",
        "        if i%100 == 0:\n",
        "          print(f\"i am running {i} for feature extraction\")\n",
        "          features_list.append(process_chunk(X_train.loc[i]))\n",
        "\n",
        "      data = pd.DataFrame.from_records(features_list,index=index_x)\n",
        "\n",
        "      data.index.name = \"time\"\n",
        "      # data.to_csv(data_file)\n",
        "\n",
        "    base_model = XGBClassifier(\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='auc',\n",
        "    random_state=42,\n",
        "\n",
        ")\n",
        "\n",
        "    pipeline = Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('clf', base_model)\n",
        "    ])\n",
        "\n",
        "    param_grid = {\n",
        "        'clf__n_estimators': [100, 200],\n",
        "        'clf__max_depth': [3, 5, 10],\n",
        "        'clf__learning_rate': [0.01, 0.1,0.2],\n",
        "        'clf__subsample': [0.8, 1.0],\n",
        "        'clf__colsample_bytree': [0.8, 1.0]\n",
        "    }\n",
        "\n",
        "    grid = GridSearchCV(\n",
        "        estimator=pipeline,\n",
        "        param_grid=param_grid,\n",
        "        scoring='roc_auc',\n",
        "        cv=3,\n",
        "        n_jobs=-1,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    grid.fit(data, y_train)\n",
        "\n",
        "    best_model = grid.best_estimator_\n",
        "    os.makedirs(model_directory_path, exist_ok=True)\n",
        "    #best_model.save_model('model.json')\n",
        "    joblib.dump(best_model, os.path.join(model_directory_path, 'model.joblib'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7n-jboJH-0fU"
      },
      "source": [
        "### The `infer()` Function\n",
        "\n",
        "In the inference function, your trained model (if any) is loaded and used to make predictions on test data.\n",
        "\n",
        "**Important workflow:**\n",
        "1. Load your model;\n",
        "2. Use the `yield` statement to signal readiness to the runner;\n",
        "3. Process each dataset one by one within the for loop;\n",
        "4. For each dataset, use `yield prediction` to return your prediction.\n",
        "\n",
        "**Note:** The datasets can only be iterated once!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-18T10:03:59.120294Z",
          "start_time": "2024-11-18T10:03:59.114830Z"
        },
        "id": "r1b7hRkl-0fU"
      },
      "outputs": [],
      "source": [
        "# def infer(\n",
        "#     X_test: typing.Iterable[pd.DataFrame],\n",
        "#     model_directory_path: str,\n",
        "# ):\n",
        "#     model = joblib.load(os.path.join(model_directory_path, 'model.joblib'))\n",
        "\n",
        "#     yield  # Mark as ready\n",
        "\n",
        "#     # X_test can only be iterated once.\n",
        "#     # Before getting the next dataset, you must predict the current one.\n",
        "#     for dataset in X_test:\n",
        "#         # Baseline approach: Compute t-test between values before and after boundary point\n",
        "#         # The negative p-value is used as our score - smaller p-values (larger negative numbers)\n",
        "#         # indicate more evidence against the null hypothesis that distributions are the same,\n",
        "#         # suggesting a structural break\n",
        "#         def t_test(u: pd.DataFrame):\n",
        "#             return -scipy.stats.ttest_ind(\n",
        "#                 u[\"value\"][u[\"period\"] == 0],  # Values before boundary point\n",
        "#                 u[\"value\"][u[\"period\"] == 1],  # Values after boundary point\n",
        "#             ).pvalue\n",
        "\n",
        "#         prediction = t_test(dataset)\n",
        "#         yield prediction  # Send the prediction for the current dataset\n",
        "\n",
        "#         # Note: This baseline approach uses a t-test to compare the distributions\n",
        "#         # before and after the boundary point. A smaller p-value (larger negative number)\n",
        "#         # suggests stronger evidence that the distributions are different,\n",
        "#         # indicating a potential structural break.\n",
        "\n",
        "\n",
        "def infer(\n",
        "    X_test: typing.Iterable[pd.DataFrame],\n",
        "    model_directory_path: str,\n",
        "):\n",
        "    # try:\n",
        "    #   !pip install pykalman\n",
        "    # except:\n",
        "    #   pass\n",
        "    # Load the trained model\n",
        "    model = joblib.load(os.path.join(model_directory_path, 'model.joblib'))\n",
        "\n",
        "    yield  # Mark as ready for inference\n",
        "\n",
        "    for dataset in X_test:\n",
        "        # Feature extraction using the same logic as in training\n",
        "        features = process_chunk(dataset)\n",
        "        features_df = pd.DataFrame([features])\n",
        "\n",
        "        # Predict probability of the positive class (1)\n",
        "        prediction = model.predict_proba(features_df)[0, 1]\n",
        "\n",
        "        yield prediction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1W0Kl9CA-0fU"
      },
      "source": [
        "## Local testing\n",
        "\n",
        "To make sure your `train()` and `infer()` function are working properly, you can call the `crunch.test()` function that will reproduce the cloud environment locally. <br />\n",
        "Even if it is not perfect, it should give you a quick idea if your model is working properly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDZeP-4--0fU",
        "outputId": "6d043bda-1772-4eab-be06-b9c5f8a8589d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ignoring cell #19: invalid syntax (<unknown>, line 2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "23:55:30 forbidden library: pykalman\n",
            "23:55:30 forbidden library: google\n",
            "23:55:30 \n",
            "23:55:30 started\n",
            "23:55:30 running local test\n",
            "23:55:30 internet access isn't restricted, no check will be done\n",
            "23:55:30 \n",
            "23:55:31 starting unstructured loop...\n",
            "23:55:31 executing - command=infer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data/X_train.parquet: download from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/146/X_train.parquet (204327238 bytes)\n",
            "data/X_train.parquet: already exists, file length match\n",
            "data/X_test.reduced.parquet: download from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/146/X_test.reduced.parquet (2380918 bytes)\n",
            "data/X_test.reduced.parquet: already exists, file length match\n",
            "data/y_train.parquet: download from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/146/y_train.parquet (61003 bytes)\n",
            "data/y_train.parquet: already exists, file length match\n",
            "data/y_test.reduced.parquet: download from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/146/y_test.reduced.parquet (2655 bytes)\n",
            "data/y_test.reduced.parquet: already exists, file length match\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "23:55:58 checking determinism by executing the inference again with 30% of the data (tolerance: 1e-08)\n",
            "23:55:58 executing - command=infer\n",
            "23:56:09 determinism check: passed\n",
            "23:56:09 save prediction - path=data/prediction.parquet\n",
            "23:56:09 ended\n",
            "23:56:09 duration - time=00:00:38\n",
            "23:56:09 memory - before=\"1.41 GB\" after=\"1.41 GB\" consumed=\"4.6 MB\"\n"
          ]
        }
      ],
      "source": [
        "crunch.test(\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bV_5CKs--0fU"
      },
      "source": [
        "## Results\n",
        "\n",
        "Once the local tester is done, you can preview the result stored in `data/prediction.parquet`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ly5q68sA-0fU"
      },
      "outputs": [],
      "source": [
        "prediction = pd.read_parquet(\"data/prediction.parquet\")\n",
        "prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oP-NLGh-0fU"
      },
      "source": [
        "### Local scoring\n",
        "\n",
        "You can call the function that the system uses to estimate your score locally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RyCrjpzv-0fU"
      },
      "outputs": [],
      "source": [
        "# Load the targets\n",
        "target = pd.read_parquet(\"data/y_test.reduced.parquet\")[\"structural_breakpoint\"]\n",
        "\n",
        "# Call the scoring function\n",
        "sklearn.metrics.roc_auc_score(\n",
        "    target,\n",
        "    prediction,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AE1i3pR-0fV"
      },
      "source": [
        "# Submit your Notebook\n",
        "\n",
        "To submit your work, you must:\n",
        "1. Download your Notebook from Colab\n",
        "2. Upload it to the platform\n",
        "3. Create a run to validate it\n",
        "\n",
        "### >> https://hub.crunchdao.com/competitions/structural-break/submit/notebook\n",
        "\n",
        "![Download and Submit Notebook](https://raw.githubusercontent.com/crunchdao/competitions/refs/heads/master/documentation/animations/download-and-submit-notebook.gif)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}